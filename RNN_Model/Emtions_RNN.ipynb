{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading/Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/mizhap/.local/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/mizhap/.local/lib/python3.11/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/mizhap/.local/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/mizhap/.local/lib/python3.11/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /home/mizhap/.local/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/mizhap/.local/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/mizhap/.local/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/mizhap/.local/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mizhap/.local/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/mizhap/.local/lib/python3.11/site-packages (from nltk) (4.66.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mizhap/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_Word2Vec_selection import train_test_split\n",
    "from gensim.model_Word2Vecs import Word2Vec\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import re\n",
    "import string\n",
    "\n",
    "# nltk stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#loading the dataset\n",
    "df = pd.read_csv('../dataset/emotions.csv')\n",
    "\n",
    "# tokenization and stopwords\n",
    "def clean_and_tokenize(text):\n",
    "    return [word for word in text.split() if word.lower() not in stop_words]\n",
    "\n",
    "df['tokenized'] = df['text'].apply(clean_and_tokenize)\n",
    "\n",
    "# Train Word2Vec\n",
    "model_Word2Vec = Word2Vec(sentences=df['tokenized'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to create sentence vectors by averaging word vectors\n",
    "def sentence_vector(sentence):\n",
    "    #creates word vectors. Checks if the word exists in the w2v model_Word2Vecs vocabulary\n",
    "    vectors = [model_Word2Vec.wv[word] for word in sentence if word in model_Word2Vec.wv]\n",
    "    #if if vectors is not empty put in the average vector\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    #otherwise put in zeros\n",
    "    else:\n",
    "        return np.zeros(model_Word2Vec.vector_size)\n",
    "\n",
    "X = np.array([sentence_vector(sentence) for sentence in df['tokenized']])\n",
    "\n",
    "y = df['label']\n",
    "\n",
    "#One hot encode the labels\n",
    "y_encoded = to_categorical(y, num_classes=6)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train, maxlen=20)\n",
    "X_test_padded = pad_sequences(X_test, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "max_length = df['tokenized'].apply(lambda x: len(x)).max()\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27580506,  0.88342899,  0.00157142,  0.58292574,  0.93032235,\n",
       "        0.01089578,  0.42091909,  0.36756992, -0.64774066,  0.22177424,\n",
       "       -0.69263035, -0.55858701,  0.62164718,  0.16056427,  0.05239609,\n",
       "       -0.16093135,  0.59210461, -0.46135303,  0.6523816 ,  0.31251249,\n",
       "        0.22542621, -0.07573688, -0.17835747,  0.04734053,  0.53203708,\n",
       "       -0.04820093, -0.4895753 , -0.26316565, -0.12227771, -0.38372055,\n",
       "        0.41837701,  0.43012854, -0.10650428, -0.03093025, -0.3338652 ,\n",
       "        0.11545113, -0.89732176, -0.41437379, -0.26221266, -0.18717544,\n",
       "        0.27794462, -0.50342852,  0.15163994, -0.09936377,  1.02685571,\n",
       "       -0.43805838,  0.14258055, -0.5789572 , -0.84315854, -0.52676469,\n",
       "        0.87412834, -0.67753685, -0.35771728,  0.55785424, -0.88567603,\n",
       "       -0.66216666,  0.4759753 ,  0.5579192 , -0.67180985, -0.13322593,\n",
       "       -0.19987462,  0.46587709,  0.15967521, -0.26305005, -0.73773807,\n",
       "        0.00853632,  0.06831495, -0.11329015, -0.14831172,  0.47640485,\n",
       "        0.09985154, -0.15393472,  1.1019249 , -0.09731044,  0.27800792,\n",
       "        0.56202024,  0.33536047, -0.15851808, -0.40883714,  0.59833956,\n",
       "       -0.51121551, -0.79121923, -0.4714705 , -0.35177153,  0.36178842,\n",
       "        0.24988617,  0.0958395 ,  0.25686654,  0.48968014, -0.72187418,\n",
       "        0.69365299, -0.33180565,  0.40734449,  0.13294622,  0.27474591,\n",
       "        0.44428781,  0.25356439, -0.93534744,  0.35138702, -0.21494879])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizhap/.local/lib/python3.11/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.5172 - loss: 1.2701 - val_accuracy: 0.6119 - val_loss: 1.0397\n",
      "Epoch 2/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 41ms/step - accuracy: 0.6182 - loss: 1.0233 - val_accuracy: 0.5963 - val_loss: 1.0907\n",
      "Epoch 3/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 37ms/step - accuracy: 0.6352 - loss: 0.9741 - val_accuracy: 0.6418 - val_loss: 0.9706\n",
      "Epoch 4/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 35ms/step - accuracy: 0.6512 - loss: 0.9332 - val_accuracy: 0.5932 - val_loss: 1.0760\n",
      "Epoch 5/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 35ms/step - accuracy: 0.6542 - loss: 0.9207 - val_accuracy: 0.6055 - val_loss: 1.0420\n",
      "Epoch 6/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 36ms/step - accuracy: 0.6567 - loss: 0.9133 - val_accuracy: 0.6392 - val_loss: 0.9622\n",
      "Epoch 7/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 35ms/step - accuracy: 0.6669 - loss: 0.8876 - val_accuracy: 0.5363 - val_loss: 1.1751\n",
      "Epoch 8/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 36ms/step - accuracy: 0.6506 - loss: 0.9257 - val_accuracy: 0.6673 - val_loss: 0.8839\n",
      "Epoch 9/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 38ms/step - accuracy: 0.6725 - loss: 0.8704 - val_accuracy: 0.6056 - val_loss: 1.0492\n",
      "Epoch 10/10\n",
      "\u001b[1m1043/1043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 39ms/step - accuracy: 0.6685 - loss: 0.8772 - val_accuracy: 0.6497 - val_loss: 0.9817\n",
      "\u001b[1m2606/2606\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.6487 - loss: 0.9837\n",
      "Test Loss: 0.9830063581466675\n",
      "Test Accuracy: 0.6496365070343018\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Reshape\n",
    "\n",
    "num_classes = 6\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(Reshape((100, 1), input_shape=(100,)))\n",
    "rnn.add(SimpleRNN(units=64))  # Add your desired RNN layer (e.g., SimpleRNN)\n",
    "rnn.add(Dense(units=num_classes, activation='softmax'))  # Add output layer with appropriate activation\n",
    "\n",
    "# Compile the model\n",
    "rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# Train the model\n",
    "rnn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = rnn.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.save('emotions_rnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizhap/.local/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('emotions_rnn.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
